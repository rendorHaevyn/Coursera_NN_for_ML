INDEX:
== WEEK 1 - INTRODUCTION ==
== WEEK 2 - NEURAL NETWORK ARCHITECTURE ==
== WEEK 3 - NEURAL NETWOKS FOR MACHINE LEARNING ==




== WEEK 1 - INTRODUCTION ==
** We can envisage 5 types of idealised neurons **

Linear Neurons:
> y = b + Σx(i)w(i)

Binary Threshold Neurons (ie: 0 or 1 if greater than threshold):
> activiation threshold = θ
> z = b + Σx(i)w(i)
> y = {1 if z>= θ, else 0

Rectified Linear Neurons / Units:
> compute linear weighted sum of their inputs, and output a non-linear function of the total input
> activiation threshold = θ
> z = b + Σx(i)w(i)
> y = {z if z>= θ, else 0
> we can also treat ReLU output as a Poisson rate for activation, rather than a real number

Sigmoid Neurons:
> return a real-valued output, that is a smooth and bounded function of their total input 
> typically use logistic function
> returns real value
> z = b + Σx(i)w(i)
> y = 1 / (1 + e^-z)
> ie: if z is large: y --> 1; z is 0: y --> 0.5; z is small: y --> 0

Stochastic Binary Neurons:
> use the same algorithm as sigmioid nerons (logistic), however, convert the output into a probability, and use that to make a binary decision / classification
> activiation threshold = θ
> returns probability, which becomes binary decision
> z = b + Σx(i)w(i)
> y = {1 if (1 / (1 + e^-z)) > θ, else 0)

Tanh Neuron:
> as above, except producing a value in range {-1,1}
> activiation threshold = θ
> returns real value
> z = b + Σx(i)w(i)
> y = {tanh(z) if z > θ, else 0

** Types of learning **

Supervised Learning:
> Regression: The target output is a real number or vector of real numbers, ie: temperature tomorrow, price of stock in 6 months
> Classification: The target output is a class label, ie: choice between 1 or 0, or between car model
> We choose a model-class, which are defined by different algorithms
- ŷ = ƒ(x:W) 
- the model-class, ƒ, uses numerical parameters, W, to map each input vector, x, into a predicted output, ŷ
- parameters are adjusted to reduce the loss or difference between target output, y, on each training case, and the predicted output of the model, ŷ
- we often used teh squared difference for this, 1/2 (y - ŷ)^2

Unsupervised Learning:
> Aims include:
- An internal representation of the input that can be useful for subsequent supervised learning / reinforcement learning (ie: K-Means clustering)
- Provide compact, low-representation of the input (ie: PCA)


== WEEK 2 - NEURAL NETWORK ARCHITECTURE ==
** There are 3 typical architectures of neural networks **

Feed-forward Neural Networks:
> Input units layer --> hidden units layer (if > 1 layer then these are called "deep" neural networks) --> output units layer

Recurrent Neural Networks:
> Have directed cycles in their connection graph
> Complicated dynamics, difficult to train
> The hidden layers can act as time "slices", ie: might be useful for predicting the price of a stock, given the price each day over the last 5 years
- they use the same weights at every time slice and receive input at each time slice

Symmetrically Connected Neural Networks:
> Like RNNs, but that connections between units are symmetrical, having the same weight in both directions
> They cannot model cycles

** Perceptrons **
> Are a standard model for statistical pattern recognition
> Inputs --> hand-coded weights --> feature units --> learned weights --> decision unit (class decision based upon threshold)
> Biases constitute an additional feature, with an activity of 1, for which we can learn the weight

Perceptron convergence procedure:
> For each training example:
- If output is correct, leave weights as they are
- If output is incorrectly a 0, add input vector to the weights
- If output is incorrectly a 1, subtract input vector from the weights

Inputs can be regarded as constraints to the NN, inasmuch as they constrain the set of weights that give the correct classification results.


== WEEK 3 - NEURAL NETWOKS FOR MACHINE LEARNING ==

Learning Weights of a Linear Neuron:
> real valued output which is a weighted sum of their input
- ŷ = Σw(i)x(i) = w^(T).x  ==> weight vector dot-product input vector
> minimise error summed over all training cases
- E = 1/2 . Σ(y-ŷ)^2
> we apply a delta rule for learning
- Δw(i) = εx(i).(y-ŷ), where ε = learning rate

Learning Weights of a Logistic output Neuron:
> logit, z, is a bias plus sum product weight vector and input vector
- z = b + Σw(i)x(i)
- ŷ = 1 / (1 + e^-z)

BackPropagation:
> For hidden units, we can compute how fast the error changes as we change hidden activity (weights)
> We compute the error derivatives for all the hidden units simultaneously (ie: error derivative = how fast the error changes as we change the hidden activity)
> We use error derivatives in distal layers to feed back in to error derivatives in proximal layers
> The error derivatives are calculated for every weight on a single training case
> How to uses these error derivatives:
1. Optimisation
> How often to update:
- Online: update weights after each training case
- Full batch: update weights after full traing set is trained - this can be quite timely / cost ineffective
- Mini-batch: small random sample of trainng cases
> How much to update:
- fixed learning rate
- adaptive learning rate
- apply learning rate differently across connections between layers
2. Overfitting
> Training data has two types of noise:
- target values may be unreliable
- there is sampling error
> How to deal with overfitting
- weight decay, which is keeping network weights small
- weight sharing, which is keeping weights similar across features
- early stopping, which is stopping training when performance on test set gets worse
- model average, which are weights averaged across multiple trained nerual nets
- Bayesian fitting of neural nets, which is like model averaging
- dropout, which is randomly dropping hidden nodes when training





